{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 1: LLMs Experiments -- Applications -- Tasks 0 & 1\n",
    "----\n",
    "\n",
    "Due date: January 16th, 2025 @ 9pm\n",
    "\n",
    "Points: 60\n",
    "\n",
    "Goals:\n",
    "- explore and understand some of the hyperparameters available to you with LLMs\n",
    "- consider more deeply the philosophy of AI and LLMs\n",
    "\n",
    "Complete in groups of: __one (individually)__\n",
    "\n",
    "Files to submit:\n",
    "- `lmms_temperature_task01.ipynb` (25 points)\n",
    "- `lmms_temperature_task23.ipynb` (20 points)\n",
    "- `lmms_temperature_task4.ipynb` (15 points)\n",
    "    - all graphs you produced in support of task 4\n",
    "\n",
    "Allowed python modules:\n",
    "- You are not allowed to use any library's implementation of the `softmax` function. You may use the `random` and `numpy` libraries in this assignment.\n",
    "\n",
    "Instructions:\n",
    "- __Start__ by completing Quiz 1 quiz on __Canvas__.\n",
    "- Complete outlined problems in the corresponding notebooks. \n",
    "- When you have finished each notebook, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but comment it out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "Name: Kate Lanman\n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "- https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n",
    "    - Sample grouped bar chart using matplotlib.\n",
    "\n",
    "AI Collaboration\n",
    "---\n",
    "Following the *AI Collaboration Policy* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for. Additionally, provide comments in-line identifying the specific sections that you used LLMs on, if you used them towards the generation of any of your answers.\n",
    "\n",
    "Nested list comprehensions supplied by AI collaborators __must__ be re-written so that they are not double- (or triple-) nested *and* they use sensible variable names.\n",
    "\n",
    "Make sure to include both GPT-like LLMs and integrated agents such as Copilot. __Read the piazza post/policy in the syllabus for expectations of citations.__\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort?\n",
    "\n",
    "    Yes, I believe this reflects my best effort.\n",
    "2. What was/were the most challenging part(s) of the assignment?\n",
    "\n",
    "    Generating a cohesive story.\n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why?\n",
    "\n",
    "    I do not need feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Large Language Models Experiments (20 points)\n",
    "---\n",
    "\n",
    "For this task, you'll be playing around with a \"base\" large language model. Even though you'll learn the inner workings of these models later in the course, this is an important step to understanding that what we see on services like ChatGPT, Claude, etc. are __not__ \"just\" language models.\n",
    "\n",
    "For this part of the homework, you'll be accessing a __pretrained__ language moddel from `huggingface`, then playing with text generation and a couple hyperparameters. A hyperparameter is a parameter of a model that is manually set and controls some aspect of either the learning algorithm or the output algorithm.\n",
    "\n",
    "The two hyperparameters you'll focus on are __temperature__ and __top-k__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rt/y418zwhn7mv6tf_kxcm3nbtm0000gn/T/ipykernel_61981/1608905362.py\", line 6, in <module>\n",
      "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# you need both the hugging face transformers and the torch libraries for this task\n",
    "# uncomment the lines below to install them\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED\n",
    "\n",
    "def generate(prompt: str, tokenizer: AutoTokenizer, model: AutoModelForCausalLM,\n",
    "             top_k: int = None, temperature: float = 1, max_new_tokens: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    This function takes a prompt as input and generates a story using the given model. \n",
    "    The function returns the generated story as a string. \n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The prompt for the story\n",
    "    tokenizer (AutoTokenizer): The tokenizer for the model\n",
    "    model (AutoModelForCausalLM): The model to generate the story\n",
    "    top_k (int): ???? (you'll investigate this)\n",
    "    temperature (float): ???? (you'll investigate this)\n",
    "    max_new_tokens (int): The maximum number of new tokens to generate\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # https://huggingface.co/docs/transformers/generation_strategies\n",
    "    gen_tokens = model.generate(\n",
    "\n",
    "        input_ids,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        top_k=top_k,\n",
    "\n",
    "        temperature=temperature,\n",
    "\n",
    "        max_new_tokens=max_new_tokens,\n",
    "\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your overall goal is to __get the language model to produce a coherent story__. This is difficult. We don't expect perfect (or necessarily great) results, but we do expect you to experiment with different propmts.\n",
    "\n",
    "Complete the steps outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story about a girl who outlets you vote 'No' or 'Yes' on anything, and answer the following: Never\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a beginning prompt for your story and use the `generate` function to get your text. \n",
    "# Print your output below this cell.\n",
    "\n",
    "prompt = \"Write a story about a girl\"\n",
    "gen_text = generate(prompt, tokenizer, model)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story about a girl who is dying and then asks me to save her.\n",
      "\n",
      "MAGIA: The combined effect gives\n"
     ]
    }
   ],
   "source": [
    "# 2. Using the same prompt as above, use the `generate` function to get your text.\n",
    "# Print your output below this cell.\n",
    "\n",
    "gen_text2 = generate(prompt, tokenizer, model)\n",
    "print(gen_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What do your experiments from steps 1 and 2 show you? \n",
    "\n",
    "    The model is taking the prompt and predicting what words come next.\n",
    "\n",
    "Play around with different prompts until you have one you're relatively satisfied with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it's the mother and daughter's responsibility to their children.\"\n",
      "\n",
      "Since the announcement, a\n"
     ]
    }
   ],
   "source": [
    "def prompt_test(prompt):\n",
    "    print(generate(prompt, tokenizer, model))\n",
    "\n",
    "prompt_test(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a simple grain of self-improvement that everyone professed and I did a job of showcasing the golden\n"
     ]
    }
   ],
   "source": [
    "prompt_test(\"Once upon a time there was a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a girl's words of praise to be received by a Knight of Highness Highness with a feeling of\n"
     ]
    }
   ],
   "source": [
    "prompt_test(\"Once upon a time there was a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. An old man sat looking up from a long misconception that he had been sleeping with his fish sunken\n"
     ]
    }
   ],
   "source": [
    "prompt_test(\"It was a dark and stormy night.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. Lighting came out above our heads and included porn.\n",
      "\n",
      "I lay there thinking that I forgot the\n"
     ]
    }
   ],
   "source": [
    "prompt_test(\"It was a dark and stormy night.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night.\n",
      "\n",
      "The Vali, the oracle of the Oha`ti, said: \"These\n"
     ]
    }
   ],
   "source": [
    "prompt_test(\"It was a dark and stormy night.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " It was a dark and stormy night.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. Although contains a cool it had only broken off over appeal and lapped its rear end through a metal\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " sieve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-like hatch.\n",
      "\n",
      "The light grill could be positioned and could be occupied by any remaining product\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " that escaped the prior mechanisms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". The hydrogen gas can provide fuel for several purposes:\n",
      "\n",
      "Ahri operable liquefied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " metals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shed from an earthenware furnace;\n",
      "\n",
      "The water supply for tapping and washing herbs added\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a strong scent of\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " honey;\n",
      "\n",
      "And the hairdressers sang windward,\n",
      "\n",
      "With each song before\n"
     ]
    }
   ],
   "source": [
    "# 4. Programmatically build a story from your prompt, printing it out as you go.\n",
    "# Your story should include 5 user-entered components (inclusive of the initial prompt).\n",
    "# Make this into a function you can call to generate a story.\n",
    "\n",
    "# initial prompt\n",
    "prompt = input()\n",
    "response = generate(prompt, tokenizer, model)\n",
    "story = response\n",
    "\n",
    "print(response)\n",
    "\n",
    "# building on the story\n",
    "for i in range(4):\n",
    "    user_input = input()\n",
    "    prompt = response.replace(prompt, '') + ' ' + user_input # prompt is last response + user component\n",
    "    response = generate(prompt, tokenizer, model)\n",
    "\n",
    "    # relay newest line\n",
    "    print(response.replace(prompt, \"\")) \n",
    "\n",
    "    story += \" \" + user_input + \" \" + response.replace(prompt, \"\")\n",
    "    \n",
    "# WARNING: this code will take a long time to run if you pass too many tokens to \n",
    "# the `generate` function. You'll want to incorporate __part__ of the story produced\n",
    "# so far and the user's next input to help keep the token count down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a dark and stormy night. Although contains a cool it had only broken off over appeal and lapped its rear end through a metal sieve -like hatch.\\n\\nThe light grill could be positioned and could be occupied by any remaining product that escaped the prior mechanisms . The hydrogen gas can provide fuel for several purposes:\\n\\nAhri operable liquefied metals  shed from an earthenware furnace;\\n\\nThe water supply for tapping and washing herbs added a strong scent of  honey;\\n\\nAnd the hairdressers sang windward,\\n\\nWith each song before'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy + paste your story here into markdown. (So that you don't lose this output each time you restart your kernel and clear your outputs.)\n",
    "\n",
    "It was a dark and stormy night. Heraldism waited – tornados on EFigure, wildfires spreading across swamps, and several fires burning lit even the darkest corners of the city.  Typically, with their power out, embers from storms became inflaming into the fabric of the streets, but tonight something was different.  The arrests drove too close to the source of force. Firework crumbled into dustballs, Ah he understood now. Ignoring the ongoing conflict, he hurried to the place Ah magic mortally attacked, firmly and  calmly detecting that it would never happen to him. And yet, as for the moment that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP!!!\n",
    "=====\n",
    "\n",
    "Before turning any homework notebook in:\n",
    "\n",
    "- When you have finished each notebook, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0 __after__ you finish the rest of the assignment\n",
    "- Submit your work on Gradescope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
