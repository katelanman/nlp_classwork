{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 1: LLMs Experiments -- Applications -- Tasks 2 & 3\n",
    "----\n",
    "\n",
    "See the `task01` notebook for instructions. Include *all* AI citations in the `task01` notebook.\n",
    "\n",
    "Name: Kate Lanman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rt/y418zwhn7mv6tf_kxcm3nbtm0000gn/T/ipykernel_62020/1608905362.py\", line 6, in <module>\n",
      "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/katelanman/opt/anaconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# you need both the hugging face transformers and the torch libraries for this task\n",
    "# uncomment the lines below to install them\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED\n",
    "\n",
    "def generate(prompt: str, tokenizer: AutoTokenizer, model: AutoModelForCausalLM,\n",
    "             top_k: int = None, temperature: float = 1, max_new_tokens: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    This function takes a prompt as input and generates a story using the given model. \n",
    "    The function returns the generated story as a string. \n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The prompt for the story\n",
    "    tokenizer (AutoTokenizer): The tokenizer for the model\n",
    "    model (AutoModelForCausalLM): The model to generate the story\n",
    "    top_k (int): ???? (you'll investigate this)\n",
    "    temperature (float): ???? (you'll investigate this)dd\n",
    "    max_new_tokens (int): The maximum number of new tokens to generate\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # https://huggingface.co/docs/transformers/generation_strategies\n",
    "    gen_tokens = model.generate(\n",
    "\n",
    "        input_ids,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        top_k=top_k,\n",
    "\n",
    "        temperature=temperature,\n",
    "\n",
    "        max_new_tokens=max_new_tokens,\n",
    "\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Top-k (10 points)\n",
    "---\n",
    "\n",
    "Next, we'll investigate the `top_k` parameter. `None` means that you are not using `top-k` sampling.\n",
    "\n",
    "For each out the following `top-k` settings, run `generate` with these values specified for `top-k` __twice__.\n",
    "\n",
    "Fill in this table:\n",
    "\n",
    "__Prompt:__ It was a dark and stormy night.\n",
    "\n",
    "| `top_k` value | output 1 | output 2 |\n",
    "| - | - | - |\n",
    "| `None` | It was a dark and stormy night. The fog was thumping. I walked round and round and then slowly pulled myself up to those shadows | It was a dark and stormy night. Sweeping through snowy mountains and misted fields and colder fall pestered, angels convened inside the likely |\n",
    "| `1` | It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were | It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were |\n",
    "| `2` | It was a dark and stormy night. \"I was just sitting there and I was like, 'Oh my god, I can | It was a dark and stormy night. The sun was setting, but the wind was blowing. I was in a hurry to get out of |\n",
    "| `5` | It was a dark and stormy night. The sky was full of people, but it was not dark enough to see them. I could see | It was a dark and stormy night. The sky was filled with the sounds of the wind. \"I'm sorry, I didn|\n",
    "| `10` | It was a dark and stormy night. It was a cold winter night, and I had the worst of it. I was standing in this | It was a dark and stormy night. The storm was heavy and the wind whipped the sky in the direction of the village. |\n",
    "| `50` | It was a dark and stormy night. There was a lot of rain,\" Mr. C.M. said. The forecast showed | It was a dark and stormy night. The stars that rolled in the sky were lit by the great golden flames that were burning in every corner |\n",
    "| `100` | It was a dark and stormy night. An angel's voice said, \"Look out for the kids. This is danger.\" I turned to | It was a dark and stormy night. An explosion. A car ran over. Someone shot it at close range. It was probably going to |\n",
    "\n",
    "1. What is the effect of having a `top-k` value of 1? \n",
    "\n",
    "    The model returns the same result.\n",
    "2. What value of `top-k` do you prefer? Why? \n",
    "\n",
    "    top-k = 10. The responses are the closest to sounding coherent and like the start of a story. There is some continuity without it sounding redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. The scene where the death of an innocent man had suddenly coalesced into more than one world conflict watching\n",
      "It was a dark and stormy night. Clouds covered the shore and clouds of the Rhine. A terrible moon was rising once more and now\n",
      "\n",
      "k = 1\n",
      "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were\n",
      "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were\n",
      "\n",
      "k = 2\n",
      "It was a dark and stormy night. The sun was shining, but the clouds had not yet begun to fall, and the wind had not\n",
      "It was a dark and stormy night. The sun was setting, and the clouds were falling. The wind was blowing in the direction of the\n",
      "\n",
      "k = 5\n",
      "It was a dark and stormy night.\n",
      "\n",
      "A man in his early 50's was standing by his home.\n",
      "\n",
      "He was wearing\n",
      "It was a dark and stormy night.\n",
      "\n",
      "\"It was dark, and the rain was pouring down from the sky.\"\n",
      "\n",
      "The\n",
      "\n",
      "k = 10\n",
      "It was a dark and stormy night. We walked to the house where the fire had spread from the roof, where the family had gathered.\n",
      "It was a dark and stormy night. And then I heard a knock at the door. I was startled by the sound of a door knocking\n",
      "\n",
      "k = 50\n",
      "It was a dark and stormy night. It was early in late July at an hour early as the heat was rising back up to the top\n",
      "It was a dark and stormy night. We had some big drinks - there was no telling where we were going to leave off so we turned\n",
      "\n",
      "k = 100\n",
      "It was a dark and stormy night. The streets were littered with redheads, but fortunately they were still open. Two tall, slender black\n",
      "It was a dark and stormy night. He'd watched for over three hours that night when he was hit the other day in a fall from\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any code that you want to write for top-k investigations here\n",
    "prompt = \"It was a dark and stormy night.\"\n",
    "for k in [None, 1, 2, 5, 10, 50, 100]:\n",
    "    print(f\"k = {k}\")\n",
    "\n",
    "    print(generate(prompt, tokenizer, model, top_k=k))\n",
    "    print(generate(prompt, tokenizer, model, top_k=k))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Temperature (10 points)\n",
    "---\n",
    "\n",
    "Next, we'll investigate the `temperature` parameter. `1.0` means that you are not using temperature (effectively).\n",
    "\n",
    "For each out the following `temperature` settings, run `generate` with the same prompt and these values specified for `temperature` __twice__.\n",
    "\n",
    "Write down your prompt, then fill in this table:\n",
    "\n",
    "__Prompt:__ It was a dark and stormy night.\n",
    "\n",
    "| `temperature` value | output 1 | output 2 |\n",
    "| - | - | - |\n",
    "| `0.01` | It was a dark and stormy night. The sun was shining brightly, and the wind was blowing. The wind was blowing, and the wind | It was a dark and stormy night. The sun was shining brightly, and the wind was blowing. The wind was blowing, and the wind |\n",
    "| `0.5` | It was a dark and stormy night. The walls of the house were covered with leaves and leaves, and there were no windows. The windows | It was a dark and stormy night. The winds were blowing in from the west. The police said they had been called to the |\n",
    "| `1.0` | It was a dark and stormy night. When he stood up he had a mighty dream. \\*\\*\\*\\*\\* These things have | It was a dark and stormy night. The streetlights were streaked with dirty blood, and four people lay dead in the little corner of |\n",
    "| `1.5` | It was a dark and stormy night. Twilight have just lost patience totaled Published: 8 June 1992 morningtwitter hashtagrulybayetc 3 in the | It was a dark and stormy night. Shroud Wade Turan owned stamp Lot 5684 Huehorje man entered deck shack glowing tranquil green string |\n",
    "| `3.0` | It was a dark and stormy night. Suff skyline details 1500 Gentlerit tiny charm murdereight windstruct Kearkilledaled bury Gam strain apostleient | It was a dark and stormy night. Lucy sell horde bis utter dispersed ego princes Regist mediation agriculture plans isn __ background articles management=~criptions________________________ |\n",
    "\n",
    "1. What is the effect of having a `temperature` value close to 0? \n",
    "\n",
    "    The response is redundant.\n",
    "2. What about a `temperature` value above 1? \n",
    "    \n",
    "    The response is incoherent and not all English words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 0.01\n",
      "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were\n",
      "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were\n",
      "\n",
      "temperature = 0.5\n",
      "It was a dark and stormy night. The wind was blowing, and the trees were falling down. The wind was heavy. The wind was\n",
      "It was a dark and stormy night. I was sitting in the back of my car. I was trying to figure out what Handlebar was\n",
      "\n",
      "temperature = 1.0\n",
      "It was a dark and stormy night. Television seemed to contain a very large amount of dead bodies outside. Although Lee passed peacefully with no pictures\n",
      "It was a dark and stormy night. It cleared immediately and as soon as I could reach home I called one of my quarters and he's\n",
      "\n",
      "temperature = 1.5\n",
      "It was a dark and stormy night. Bob Melvin bundled them up full res Grim scLoSalsethhello 238 1931 Freyland resin Tur\n",
      "It was a dark and stormy night.It was as forced separating and intoxicating incessantly as meth joints trying pleasant communications Frequency Fact Avg char\n",
      "\n",
      "temperature = 3.0\n",
      "It was a dark and stormy night. Magicka Armor comparisons exceeds Blizzard we Farrell tact usage deserveprisecontained zoopoliceん revised ingenuity messageswantir\n",
      "It was a dark and stormy night. Nan h Mem worked TigSold materials strokes testing Numbers creates Bravauer mig ostools destined amounts canvas Quentin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any code that you want to write for temperature investigations here\n",
    "for temp in [0.01, 0.5, 1.0, 1.5, 3.0]:\n",
    "    print(f\"temperature = {temp}\")\n",
    "    \n",
    "    print(generate(prompt, tokenizer, model, temperature=temp))\n",
    "    print(generate(prompt, tokenizer, model, temperature=temp))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP!!!\n",
    "=====\n",
    "\n",
    "Before turning any homework notebook in:\n",
    "\n",
    "- When you have finished each notebook, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0 __after__ you finish the rest of the assignment\n",
    "- Submit your work on Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
